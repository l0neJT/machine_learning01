# Predicting Exercise Quality
## [John's Hopkins University Practical Machine Learning on Coursera][coursera_machine_learning]
Logan J Travis
2014-08-24

### Executive Summary
**WRITE**

### Get Data
Download training and test datasets to ./data/ if not present. Data provided as part of the [John's Hopkins University Practical Maching Learning][coursera_machine_learning] class on [Coursera][coursera] by [Groupware@LES][groupwareLES] from their [Human Activity Recognition research][data_source] project.

```{r getData, echo = FALSE, }
# Load libraries
library(caret)
library(plyr)

# Create data directory if missing
if(!file.exists("./data/")) dir.create("./data/")

# Download training dataset if missing
# Writes download detail with url, file name, and timestamp
# Prints file name to console
destfile <- "./data/pml-training.csv"
if(!file.exists(destfile)) {
    url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
    download.file(url = url, destfile = destfile, method = "curl")
    downloadDetail <- data.frame(url = url, destfile = destfile, time = Sys.time())
    write.csv(downloadDetail, file = "./data/pml-training-download-detail.txt")
    print(paste("File downloaded to", destfile))
} else print(paste("File found at", destfile))

# Download testing dataset if missing
# Writes download detail with url, file name, and timestamp
# Prints file name to console
destfile <- "./data/pml-testing.csv"
if(!file.exists("./data/pml-testing.csv")) {
    url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
    download.file(url = url, destfile = destfile, method = "curl")
    downloadDetail <- data.frame(url = url, destfile = destfile, time = Sys.time())
    write.csv(downloadDetail, file = "./data/pml-testing-download-detail.txt")
    print(paste("File downloaded to", destfile))    
} else print(paste("File found at", destfile))

# Cleanup variables
rm(destfile)
```

### Explore Training Dataset
**WRITE** Summary

#### Explanation for "new" window rows
The [paper][data_paper] submitted by [Groupware@LES][groupwareLES] details their sliding window method in section 5.1 Feature extraction and selection. They calculated statiscal measures for windows ranging from 0.5 to 2.5 seconds with 0.5 seconds of overlap (excluding 0.5 second window).

However, the test data does not utlize such windows and includes only instantaneous measures. This does not align with the methodology explored in the paper though it does match their ultimate goal: continuous feedback to users. The models developed in this paper will attempt to acheive that goal so do not utilize the statistical measures as predictors.

#### Reading/Spliting Data
**WRITE** Summary

```{r readTrain, cache = TRUE, echo = FALSE}
# Set seed
set.seed(296785)

# Establish column classes to skip statistical meaures noted above
# Sets 'X' column as character to prevent error
colClasses <- c("character", "factor", rep("numeric", 2), "character", "factor", 
                rep("numeric", 5), rep("NULL", 25), rep("numeric", 13), 
                rep("NULL", 10), rep("numeric", 9), rep("NULL", 15), 
                rep("numeric", 3), rep("NULL", 15), "numeric", rep("NULL", 10), 
                rep("numeric", 12), rep("NULL", 15), "numeric", rep("NULL", 10), 
                rep("numeric", 9), "factor")

# Read training dataset
# Converts 'X' column to numeric
dat <- read.csv("./data/pml-training.csv", colClasses = colClasses)
dat$X <- as.numeric(dat$X)

# Split into training and control
# Limits columns to those used for predictions/results
inTrn <- createDataPartition(dat$classe, p = 0.75, list = FALSE)
trn <- dat[inTrn, -c(1, 3:7)]
ctl <- dat[-inTrn, -c(1, 3:7)]

# Print structur for 'trn' data
print(str(trn))
```

#### Ploting Features Grouped by User
**WRITE** Summary

<ul>
    <li>Belt Features<br>
    ```{r plotBelt, cache = TRUE, echo = FALSE}
    featurePlot(x = trn[, c(2:5, 54)], y = trn[, 1], plot = "pairs")
    ```
    </li>
    <li>Arm Features<br>
    ```{r plotArm, cache = TRUE, echo = FALSE}
    featurePlot(x = trn[, c(15:18, 54)], y = trn[, 1], plot = "pairs")
    ```
    </li>
    <li>Dumbell Features<br>
    ```{r plotDbell, cache = TRUE, echo = FALSE}
    featurePlot(x = trn[, c(28:31, 54)], y = trn[, 1], plot = "pairs")
    ```
    </li>
    <li>Forearm Features<br>
    ```{r plotFore, cache = TRUE, echo = FALSE}
    featurePlot(x = trn[, c(41:44, 54)], y = trn[, 1], plot = "pairs")
    ```
    </li>
</ul>

#### Modeling Trees
**WRITE** Summary

* Standard Tree (rpart)
    ```{r modelTree, echo = FALSE, cache = TRUE}
    # Set control for train function
    # Uses k-fold cross-validation
    tr <- trainControl(method = "cv", number = 4, allowParallel = TRUE)
    
    # Create tree model and print confusion matrix
    model <- train(classe ~ ., data = trn, method = "rpart", trControl = tr)
    print(confusionMatrix(predict(model, trn), trn$classe)$overall)
    
    # Cleanup variables
    rm(tr, model)
    ```

* Bagged Tree (treebag)
    ```{r modelTreebag, echo = FALSE, cache = TRUE}
    # Set control for train function
    # Uses k-fold cross-validation
    tr <- trainControl(method = "cv", number = 4, allowParallel = TRUE)

    # Create tree model and print confusion matrix
    model <- train(classe ~ ., data = trn, method = "treebag", trControl = tr)
    print(confusionMatrix(predict(model, trn), trn$classe)$overall)
    
    # Cleanup variables
    rm(tr, model)
    ```

#### Comparing to Random Forrest
**WRITE* Summary

```{r modelRF, echo = FALSE}
# Set control for train function
# Uses out-of-bag cross-validation
tr <- trainControl(method = "oob", number = 4, allowParallel = TRUE)

# Create tree model and print confusion matrix
model <- train(classe ~ ., data = trn, method = "rf", trControl = tr)
print(confusionMatrix(predict(model, trn), trn$classe)$overall)
```

#### Determing Next Steps
**WRITE**

* Non-linear
* Poor tree performance (without bagging) on all classes
* Model-based approach might make sense with real-world testing data. However, prior probability for each error unavailable in dataset; errors created intentionally.
* Use random forrest or bagged tree

### Establish Final Model
**WRITE** Summary

```{r modelFinal, echo = FALSE}
# Calculated in 'testRF code block
print(model)
plot(model$finalModel, main = "Final Model: Random Forest")
```

### Test Against Control Data
**WRITE** Summary

```{r modelTest, echo = FALSE}
# Predict control results and print confusion matrix
ctlPred <- predict(model, ctl)
print(confusionMatrix(ctlPred, ctl$classe))

# Plot accuracy by point size by taking log10(count)
ctlPlot <- ddply(data.frame(ctl, ctlPred), .(user_name, classe, ctlPred),
                 summarize, count = length(ctlPred))
plot <- qplot(classe, ctlPred, data = ctlPlot, size = log10(count),
              ylab = "prediction", main = "Test Against Control")
plot + geom_abline(slope = 1, intercept = 0, linetype = "dotted")

# Cleanup variables
rm(ctlPred, ctlPlot, plot)
```


[coursera]: https://www.coursera.org/
[coursera_machine_learning]: https://www.coursera.org/course/predmachlearn
[groupwareLES]: http://groupware.les.inf.puc-rio.br/
[data_paper]: http://groupware.les.inf.puc-rio.br/public/papers/2013.Velloso.QAR-WLE.pdf
[data_source]: http://groupware.les.inf.puc-rio.br/har
[data_train]: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv
[data_test]: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv