# Predicting Exercise Quality
## [John's Hopkins University Practical Machine Learning on Coursera][coursera_machine_learning]
Logan J Travis
2014-08-24

### Executive Summary
**WRITE**

### Get Data
Download training and test datasets to ./data/ if not present. Data provided as part of the [John's Hopkins University Practical Maching Learning][coursera_machine_learning] class on [Coursera][coursera] by [Groupware@LES][groupwareLES] from their [Human Activity Recognition research][data_source] project.

```{r getData, echo = FALSE, }
# Load libraries
library(caret)

# Create data directory if missing
if(!file.exists("./data/")) dir.create("./data/")

# Download training dataset if missing
# Writes download detail with url, file name, and timestamp
# Prints file name to console
destfile <- "./data/pml-training.csv"
if(!file.exists(destfile)) {
    url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
    download.file(url = url, destfile = destfile, method = "curl")
    downloadDetail <- data.frame(url = url, destfile = destfile, time = Sys.time())
    write.csv(downloadDetail, file = "./data/pml-training-download-detail.txt")
    print(paste("File downloaded to", destfile))
} else print(paste("File found at", destfile))

# Download testing dataset if missing
# Writes download detail with url, file name, and timestamp
# Prints file name to console
destfile <- "./data/pml-testing.csv"
if(!file.exists("./data/pml-testing.csv")) {
    url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
    download.file(url = url, destfile = destfile, method = "curl")
    downloadDetail <- data.frame(url = url, destfile = destfile, time = Sys.time())
    write.csv(downloadDetail, file = "./data/pml-testing-download-detail.txt")
    print(paste("File downloaded to", destfile))    
} else print(paste("File found at", destfile))

# Cleanup variables
rm(destfile)
```

### Explore Training Dataset
**WRITE** Summary

#### Explanation for "new" window rows
The [paper][data_paper] submitted by [Groupware@LES][groupwareLES] details their sliding window method in section 5.1 Feature extraction and selection. They calculated statiscal measures for windows ranging from 0.5 to 2.5 seconds with 0.5 seconds of overlap (excluding 0.5 second window).

However, the test data does not utlize such windows and includes only instantaneous measures. This does not align with the methodology explored in the paper though it does match their ultimate goal: continuous feedback to users. The models developed in this paper will attempt to acheive that goal so do not utilize the statistical measures as predictors.

#### Read Training Dataset
```{r readTrain, cache = TRUE, echo = FALSE}
# Establish column classes to skip statistical meaures noted above
# Sets 'X' column as character to prevent error
colClasses <- c("character", "factor", rep("numeric", 2), "character", "factor", 
                rep("numeric", 5), rep("NULL", 25), rep("numeric", 13), 
                rep("NULL", 10), rep("numeric", 9), rep("NULL", 15), 
                rep("numeric", 3), rep("NULL", 15), "numeric", rep("NULL", 10), 
                rep("numeric", 12), rep("NULL", 15), "numeric", rep("NULL", 10), 
                rep("numeric", 9), "factor")

# Read training dataset
# Converts 'X' column to numeric
train <- read.csv("./data/pml-training.csv", colClasses = colClasses)
train$X <- as.numeric(train$X)
```
**OUTPUT**

#### Plot Features Grouped by User
**WRITE** Summary

<ul>
    <li>Belt Features<br>
    ```{r plotBelt, cache = TRUE, echo = FALSE}
    featurePlot(x = train[, c(8:11, 60)], y = train[, 2], plot = "pairs")
    ```
    </li>
    <li>Arm Features<br>
    ```{r plotArm, cache = TRUE, echo = FALSE}
    featurePlot(x = train[, c(21:24, 60)], y = train[, 2], plot = "pairs")
    ```
    </li>
    <li>Dumbell Features<br>
    ```{r plotDbell, cache = TRUE, echo = FALSE}
    featurePlot(x = train[, c(34:37, 60)], y = train[, 2], plot = "pairs")
    ```
    </li>
    <li>Forearm Features<br>
    ```{r plotFore, cache = TRUE, echo = FALSE}
    featurePlot(x = train[, c(47:50, 60)], y = train[, 2], plot = "pairs")
    ```
    </li>
</ul>

#### Testing a Tree <-- remove if too long
**WRITE** Summary

```{r testTree, echo = FALSE}
treeModel <- train(train[, -c(1, 3:7)], train[, 60], method = "rpart")
treePred <- predict(treeModel, train[, -c(1, 3:7)])
print(confusionMatrix(treePred, train[, 60]))
```

**CODE** Boosted

#### Comparison to Random-Forrest
**CODE** --> This may take a long time

#### Conclusions and Next Steps
**WRITE**

* Non-linear
* Horrendous forrest performance (without boosting) on errors types C and D
* Model-based approach might make sense with real-world testing data. However, prior probability for each error unavailable in dataset; errors created intentionally.

### Establish Model
**WRITE** Summary

**CODE** with cross-validation using left-one-out; too few samples for random subsetting or k-fold
```{r model, echo = FALSE}
# Set seed
set.seed(296785)
```

### Test

### Links
[coursera]: https://www.coursera.org/
[coursera_machine_learning]: https://www.coursera.org/course/predmachlearn
[groupwareLES]: http://groupware.les.inf.puc-rio.br/
[data_paper]: http://groupware.les.inf.puc-rio.br/public/papers/2013.Velloso.QAR-WLE.pdf
[data_source]: http://groupware.les.inf.puc-rio.br/har
[data_train]: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv
[data_test]: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv